---
title: 'Human Systems Integration'
date: 2021-07-01
permalink: /posts/2021/07/HSI
tags:
  - Interdisciplinary
  - Coursework
  - Psychology
  - Cognitive Systems Engineering
---



In December 2020, I graduated with a Bachelor's of Science in Human Systems Integration (HSI), the first bachelor's degree of its kind to to be awarded from Ohio State University.


## What is Human Systems Integration?
HSI is traditionally understood as a multi-disciplinary field of study drawing elements from human factors engineering, system safety, cognitive psychology, and systems engineering. It has deep roots in the Department of Defense (DoD) acquisition process, and is considered integral part of a holistic approach to systems development. In this light, it is also understood to be a process by which human capabilities and limitations are effectively integrated. The [Army](https://www.acqnotes.com/Attachments/HSI%20and%20ESOH%20Handbook%20for%20Pre%20MS%20A%20JCIDS%20and%20AoA%20Activities.pdf?_ga=2.130816399.1695205953.1625250278-595137530.1625250278), [Navy](https://nps.edu/documents/104395560/0/hsi_Masters_brochure_web_150427.pdf/42354142-4937-4155-a2f2-4346589438ff?t=1446069280000), and [Air Force](https://www.acqnotes.com/Attachments/Air%20Force%20Human%20System%20Integration%20Handbook.pdf?_ga=2.138026672.1695205953.1625250278-595137530.1625250278) recognize the importance of HSI and cite it as a key aspect of developing capabilities.


For the purposes of my degree, HSI is a term referring to the interdisciplinary field of study concerned with the behavior of humans and machines performing joint activity, including relationships such as the limitations of agents (physical, cognitive, or otherwise), system resource availability, resource dependency between agents, delegation of authority & responsibility, and others. These aspects go beyond traditional human factors to include aspects of control theory, computer science, and resilience engineering, emphasizing the complexity of work and highlighting an interaction and interdependence focused approach to building advanced systems.


### Traditional HSI and Human Factors Leave Something to Be Desired
Increasing popularity of HSI and related ideas is certainly a positive force to support the development of more advanced technologies. That being said, traditional HF views associated with HSI lag greatly behind the state of the art, often considering the purpose of their endeavors to increase automation at all costs in order to eliminate "human error". This notion is rebutted by [Woods and Sarter](https://apps.dtic.mil/sti/pdfs/ADA492127.pdf) and several others. One such HSI tradition is that of the US Navy, whose Human Systems Integration program was initially founded to increase the capabilities of autonomous machines. Scholars have noted that this idea holds significant philosophical baggage, almost completely ignoring the complexities and interdependence inherent in multi-agent teamwork [(Bradshaw et al., 2013)](https://www.researchgate.net/publication/260304859_The_Seven_Deadly_Myths_of_Autonomous_Systems). There are numerous examples where this philosophy fell short of expectations, such as the [crash of the USS John McCain]((https://features.propublica.org/navy-accidents/us-navy-crashes-japan-cause-mccain/). Clunky software controls allowed already understaffed and undertrained sailors to fail to realize that their control system had been reversed: the steering mechanism had been set so that port steering was on the starboard side, and vice versa. This causing the ship to make a ninety-degree turn into a 30,000 ton oil-tanker in Japan, in the middle of busiest waterway in the world. 


The full implications of automating mission-critical technologies are often overlooked when approaches like [function allocation](https://cyberleninka.org/article/n/1169062.pdf) or Sheridan & Verplanck (1978)'s levels of automation (which he later himself [criticized](https://journals.sagepub.com/doi/full/10.1177/1555343417724964) as a system design framework). While increased automation may be a desirable goal, this endeavor is one that must be approached with great caution when designing for highly complex, risky, and uncertain work domains ([Bainbridge, 1983](https://ckrybus.com/static/papers/Bainbridge_1983_Automatica.pdf), [Vicente, 2003](https://qualitysafety.bmj.com/content/qhc/12/4/291.full.pdf)). Automation can often lead to unexpected consequences and emergent interactions, especially in the presence of particularly challenging or unusual circumstances. Most automation or autonomous systems today are simply pre-programmed, leaving potential for rule-based behavior to fall short of desired outcomes as described in [Murphy & Woods, 2009](http://www.inf.ufrgs.br/~prestes/Courses/Robotics/beyond%20asimov.pdf). 


### Cognitive Systems Engineering and Resilience Engineering
Automation is by no means an inherently bad engineering choice. It often promises great gains in efficiency and increased autonomy of human agents. A perspective informed by cognitive systems engineering and resilience engineering aims to understand the complexities and pitfalls associated with automation in order to identify them and mitigate risks. I discuss this at length in [Keller & Newton, 2021], wherein Autonomous Flight Safety Systems, an emerging technology where software rules determine whether or not to terminate space launces for mission safety, are the primary focus. 
